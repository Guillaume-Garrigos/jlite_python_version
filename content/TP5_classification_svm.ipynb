{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 5 : Modélisation et résolution d'un problème de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose que l'on dispose d'un certain type de données, et on veut être capable de les **classer** en deux groupes. Ce type de problème peut être très facile à réaliser pour un humain, mais toute la question est de savoir comment automatiser cette prise de décision pour l'implémenter sur une machine.\n",
    "\n",
    "| Un problème simple de classification simple pour un humain | |\n",
    "|---|---|\n",
    "| ![](http://www.adventuresofxandd.com/wp-content/uploads/2014/06/1031-2_monster-in-a-box-2.jpg) | ![](https://static.wamiz.com/images/animals/large/naomi-6.jpg) |\n",
    "| Type de données: photos | Classes possibles : Chat ou chien |\n",
    "\n",
    "L'objectif de ce TP est de vous montrer comment résoudre ce type problème (en tout cas lorsqu'il n'est pas trop difficile). Dans un premier temps vous vous familiariserez avec le sujet sur des données synthétiques, puis vous travaillerez ensuite avec de vraies données: des photos contenant des caractères écrits à la main."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Présentation du problème**\n",
    "\n",
    "Une façon de modéliser ce problème est la suivante: on se retrouve face à une donnée $x \\in \\mathbb{R}^N$ et on veut lui attribuer une classe. Pour simplifier, lorsqu'on a un problème à deux classes comme dans les exemples ci-dessus, on dit souvent que les deux classes sont $\\{-1,+1\\}$. \n",
    "Par exemple $-1$ pourrait désigner \"chien\" et $1$ désignerait \"chat\".\n",
    "\n",
    "Ce que l'on souhaite donc implémenter est ce que l'on appelle un **classifieur**, c'est-à-dire une fonction $c : \\mathbb{R}^N \\longrightarrow \\{-1,+1\\}$ qui prend en entrée une donnée $x \\in \\mathbb{R}^N$, et qui donne en sortie une étiquette $\\pm 1$. Evidemment on ne va pas prendre n'importe quelle fonction $\\mathbb{R}^N \\longrightarrow \\{-1,+1\\}$, on veut que notre calssifieur fasse un \"bon\" travail en classant les données qu'on lui fournit.\n",
    "\n",
    "**Comment faire ça?**\n",
    "\n",
    "En pratique on suit le processus suivant: \n",
    "\n",
    "*1) Consititution d'une base de données*: on se procure\n",
    "\n",
    "- Une famille de données $\\{x_1, \\dots, x_m\\} \\subset \\mathbb{R}^N$ (par exemple tout un tas de photos de chats et de chiens)\n",
    "- La famille des étiquettes correspndantes $\\{y_1, \\dots, y_m\\} \\subset \\{ -1,+1\\}$, où chaque $y_i$ a été bien choisi par un humain\n",
    "\n",
    "Par exemple, en reprenant la convention ci-dessus, si $x_{47}$ est une photo de chien, alors $y_{47}=-1$ (rappelons que toute photo peut être vue comme un vecteur de $\\mathbb{R}^N$ où $N$ correspond au nombre de pixels de la photo).\n",
    "\n",
    "*2) La phase d'entraînement*: à partir de cette base de données, on va construire un classifieur $c : \\mathbb{R}^N \\longrightarrow \\{-1,+1\\}$, en demandant à ce que ce classifieur verifie, pour tout point de notre base de données, $c(x_i) = y_i$. Ainsi, on espère que si le classifieur fonctionne bien sur notre base de données, alors il fonctionnera également lorsqu'on lui présentera de nouvelles données.\n",
    "\n",
    "Ce TP se focalisera sur cette phase, le but étant de montrer que derrière ce problème se cache en fait un problème d'optimisation, que l'on va résoudre.\n",
    "\n",
    "*3) La phase de test*: Une fois que l'on aura trouvé notre classifieur, il faudra bien tester si il marche bien! Pour cela, on se constituera une nouvelle base de données $\\{(\\hat x_i,\\hat y_i)\\}$ similaire à celle mentionnée plus haut, et on regardera si l'étiquette prédite $c(\\hat x_i)$ est bien égale à $\\hat y_i$. On pourra par exemple compter le nombre de fois où le classifieur trouve la bonne réponse, et donc en déduire un pourcentage de succès estimé.\n",
    "\n",
    "**Rq:** Noter qu'en pratique on ne constitue pas deux bases de données. On construit une grosse base de données une fois pour toute, puis on la divise en deux parties: les données dites \"d'entrainement\" qui vont servir à construire le classifieur $c$, et d'autre part les données de \"test\" qui vont permettre d'évaluer si notre classifieur marche bien sur de nouvelles données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# I. Séparer deux groupes de points dans le plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Présentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** On commence par importer des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/donnees_entrainement.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces données correspondent à $m$ points de $\\mathbb{R}^2$, qui sont rangées dans une matrice à $m$ lignes et 2 colonnes.\n",
    "Chaque *ligne* de la matrice correspond donc à un point de $\\mathbb{R}^2$.\n",
    "Vous pouvez regarder quelle est la taille de la matrice avec la méthode `X.shape`, ce qui vous permettra de déterminer la valuer de $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu que nos données sont des points du plan, on va pouvoir facilement les visualiser avec la fonction `plt.scatter` de pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleur = [None]*m # liste vide de taille m\n",
    "for k in range(m):\n",
    "    couleur[k] = 'blue'\n",
    "_=plt.scatter(X[:, 0], X[:, 1], c=couleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Vous devriez être maintenant convaincus que ce jeu de données contient deux groupes de points appartenant à des familles distinctes. Or nous n'en savons rien, à priori, le seul moyen d'en être sur est d'aller regarder les *etiquettes* correspondantes. Importons-les:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.load('data/etiquettes_entrainement.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez que ce vecteur ne contient que des étiquettes $\\pm 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En adaptant le code de la précédente question, affichez de nouveau ce nuage de points cette fois-ci avec deux couleurs: rouge pour les points avec l'étiquette $-1$, et bleu pour les points avec l'étiquette $+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Une approche intuitive pour classer ce problème consiste à tracer une droite qui va séparer ces deux nuages: ainsi fait, on pourra dire que tout nouveau point qui apparaitra d'un côté de la droite sera \"rouge\" et de l'autre les \"bleus\".\n",
    "\n",
    "Ici on va considérer une droite dans $\\mathbb{R}^2$ comme étant l'ensemble des points satisfaisant l'équation\n",
    "\n",
    "$$ D = \\{ x \\in \\mathbb{R}^2 \\ | \\ \\langle a, x \\rangle + b =0 \\},$$\n",
    "\n",
    "où $a \\in \\mathbb{R}^2$ et $b \\in \\mathbb{R}$ sont à choisir. On notera par la suite $w = (a,b) \\in \\mathbb{R}^2 \\times \\mathbb{R}$ le vecteur de paramètres décrivant la droite $D$.\n",
    "\n",
    "En utilisant le code ci-dessous, ajoutez le tracé d'une droite aux données, et jouez avec la valeur de $w$ pour essayer de trouver la droite qui sépare le mieux les deux classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_droite(w):\n",
    "    # trace la droite des points x vérfiant l'équation <a,x>+b=0\n",
    "    # où w = (a_1, a_2, b)\n",
    "    a = w[0:2]\n",
    "    b = w[2]\n",
    "    if a[1] == 0:\n",
    "        alpha, beta = 0, 0\n",
    "    else:\n",
    "        alpha = -a[0]/a[1]\n",
    "        beta = -b/a[1]\n",
    "    abscisse = np.arange(-5,5,0.1)\n",
    "    ordonnee = alpha*abscisse + beta\n",
    "    plt.plot(abscisse, ordonnee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1, 1, 0]) # à modifier\n",
    "trace_droite(w)\n",
    "_=plt.scatter(X[:, 0], X[:, 1], c=couleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensez-vous que certaines droites sont meilleures que les autres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Modélisation : trouver une droite séparatrice optimale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "On cherche à trouver un bon paramètre $w = (a,b)\\in \\mathbb{R}^2 \\times \\mathbb{R}$, qui va décrire une bonne droite d'équation $\\langle a,x \\rangle + b= 0$.\n",
    "Mais qu'est-ce qu'une *bonne* droite? \n",
    "\n",
    "|  | Choisir une bonne droite pour séparer les données | |\n",
    "|---|---|---|\n",
    "| ![](images/classif_neutral.png) | ![](images/classif_good.png) | ![](images/classif_bad.png) |\n",
    "| Un nouveau point apparaît | Quelle droite classifie le mieux ? |  |\n",
    "\n",
    "Au vu de la question précédente, on attend deux propriétés:\n",
    "\n",
    "- D'une part on veut clairement que la droite **sépare** bien les deux groupes de données.\n",
    "- D'autre part, on voudrait que la droite passe le **plus loin possible** des données. \n",
    "  - En effet, si un nouveau point vient à apparaitre on imagine qu'il va rester près d'un des deux nuages. Donc si la droite est loin des nuages, elle sera également loin de ce nouveau point, et on évitera le problème d'un point qui apparait trop près de la droite, en qui on n'aurait pas trop confiance.\n",
    "\n",
    "Nous allons maintenant exprimer ces deux propriétés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Séparer les données\n",
    "\n",
    "La droite d'équation $\\langle a,x \\rangle + b= 0$ coupe l'espace en deux demi-espaces: les points qui vérifient $\\langle a,x \\rangle + b \\geq  0$ et ceux qui vérifient $\\langle a,x \\rangle + b \\leq 0$.\n",
    "Notre objectif est que chaque nuage (rouge et bleu) soit d'un seul côté de la droite. Il faut donc que\n",
    "- pour toute donnée $i$ telle que $y_i = +1$ (les points bleus donc), on ait $\\langle a,x_i \\rangle + b \\geq 0$,\n",
    "- pour toute donnée $i$ telle que $y_i = -1$ (les points rouges donc), on ait $\\langle a,x_i \\rangle + b \\leq 0$.\n",
    "\n",
    "Autrement dit, on veut que pour toute donnée $i$, $\\langle a,x_i \\rangle + b$ ait le même signe que $y_i$. Ce que l'on peut écrire de manière équivalente:\n",
    "\\begin{equation*}\n",
    "    (\\forall i=1,\\dots, m) \\quad y_i(\\langle a,x_i \\rangle + b) \\geq 0. \\tag{1}\n",
    "\\end{equation*}\n",
    "La propriété ci-dessus peut se réécrire comme une inégalité vectorielle $\\Phi w \\succeq 0$, pourvu qu'on introduise la bonne matrice $\\Phi$.\n",
    "En effet, si on rappelle que $w = (a,b)$ et que l'on définit $\\Phi$ par\n",
    "\\begin{equation*}\n",
    "    \\Phi :=\n",
    "    \\begin{pmatrix}\n",
    "        y_1 x_1^\\top & y_1 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        y_i x_i^\\top & y_i \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        y_m x_m^\\top & y_m\n",
    "    \\end{pmatrix} \\in \\mathcal{M}_{m,3}(\\mathbb{R}),\n",
    "\\end{equation*}\n",
    "\n",
    "on voit que le produit entre la matrice $\\Phi$ et le vecteur $w$ vaut\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    & \n",
    "    & \n",
    "    \\begin{pmatrix}\n",
    "        a \\\\\n",
    "        b\n",
    "    \\end{pmatrix}\n",
    "    \\\\\n",
    "    \\Phi w = \n",
    "    &\n",
    "    \\begin{pmatrix}\n",
    "        y_1 x_1^\\top & y_1 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        y_i x_i^\\top & y_i \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        y_m x_m^\\top & y_m\n",
    "    \\end{pmatrix} \n",
    "    & \n",
    "    \\begin{pmatrix}\n",
    "        y_1(\\langle a,x_1 \\rangle + b) \\\\\n",
    "        \\vdots  \\\\\n",
    "        y_i(\\langle a,x_i \\rangle + b) \\\\\n",
    "        \\vdots  \\\\\n",
    "        y_m(\\langle a,x_m \\rangle + b)\n",
    "    \\end{pmatrix},\n",
    "\\end{eqnarray*}\n",
    "\n",
    "et donc que la propriété (1) est équivalente à ce que tous les coefficients de $\\Phi w$ soient positifs, autrement dit, à ce que $\\Phi w \\succeq 0$. Noter que $\\Phi$ ne dépend que des données $x_i,y_i$ !\n",
    "\n",
    "**En résumé:** Une droite paramétrée par $w$ sépare bien les deux nuages si et seulement si $\\Phi w \\succeq 0$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Choisir la droite la plus éloignée des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Supposons que l'on aie une droite $D_w$ d'équation $\\langle a,x \\rangle + b =0$, paramétrée par $w =(a,b)$.\n",
    "Etant donnée une donnée quelconque $x_i$, la distance entre cette donnée $x_i$ et la droite $D_w$ se note ${\\rm{dist}}~(x_i;D_w)$.\n",
    "Ainsi, la distance entre la droite $D_w$ et le point qui en est le plus proche est simplement le minimum de ces distances:\n",
    "$$\n",
    "\\min\\limits_{i=1,\\dots, m} \\ {\\rm{dist}}~(x_i;D_w).\n",
    "$$\n",
    "En d'autres termes, la quantité ci-dessus représente la distance entre la droite $D_w$ et le nuage de points formé par les données.\n",
    "\n",
    "Puisque notre objectif est de choisir la droite la plus **éloignée** des données, on veut donc maximiser cette distance aux données. Autrement dit, on veut trouver le paramètre $w = (a,b) \\in \\mathbb{R}^3$ qui maximise cette distance, tout en respectant la contrainte de bien séparer les données (cf. paragraphe précédent):\n",
    "\\begin{equation*}\n",
    "    {\\rm Résoudre} \n",
    "    \\quad \n",
    "    \\max\\limits_{w, \\ \\Phi w \\succeq 0} \\ \\min\\limits_{i=1,\\dots, m} \\ {\\rm{dist}}~(x_i;D_w), \n",
    "    \\quad \n",
    "    \\text{ où } D_w=\\{x \\in \\mathbb{R}^2 \\ | \\ \\langle a,x \\rangle + b =0 \\}.\n",
    "\\end{equation*}\n",
    "\n",
    "Ce problème a l'air difficile! Mais il est en fait possible de montrer (cf. Feuille de TD 5) qu'il est *équivalent* à résoudre le problème suivant, qui est un peu plus sympatique:\n",
    "\\begin{equation*}\n",
    "    {\\rm Résoudre} \n",
    "    \\quad \n",
    "    \\min\\limits_{w=(a,b) \\in \\mathbb{R}^3} \\ \\Vert a \\Vert^2, \n",
    "    \\quad \n",
    "    \\text{ sous la contrainte que } \\Phi w \\succeq e, \\text{ où } e:=(1,\\dots,1)^\\top \\in \\mathbb{R}^m.\n",
    "    \\tag{P}\n",
    "\\end{equation*}\n",
    "\n",
    "**En résumé:** Pour trouver une droite séparatrice optimale, il faut minimiser $\\Vert a \\Vert^2$ sous la contrainte que $\\Phi w \\succeq e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Résolution du problème : phase d'entraînement\n",
    "\n",
    "On revient à notre problème, et on va trouver notre droite optimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Définir `e`, le vecteur $e:=(1,\\dots,1)^\\top \\in \\mathbb{R}^m$, ainsi que `Phi`, la matrice définie dans la section précédente. On rappelle que cette matrice ne dépend que de `X` et `Y`.\n",
    "\n",
    "On définira également `infini`, le vecteur $(+\\infty, \\dots, +\\infty) \\in \\mathbb{R}^m$, sachant que la quantité $+\\infty$ s'obtient avec `np.inf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Définir une fonction `objectif` qui prend en entrée un vecteur $w=(a,b) \\in \\mathbb{R}^2 \\times \\mathbb{R}$ et qui renvoie $\\Vert a \\Vert^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectif(x):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** On doit résoudre un problème d'optimisation sous contrainte. Pour cela, on va faire appel à la bibliothèque `scipy.optimize` qui peut faire cela pour nous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import LinearConstraint, minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord on doit définir la contrainte $\\Phi w \\succeq e$. Il faut pour cela utiliser la fonction `LinearConstraint` qui permet de définir une contrainte de la forme\n",
    "$$\n",
    "\\{ w \\in \\mathbb{R}^d \\ | \\ a \\preceq \\Phi w \\preceq b \\}\n",
    "$$\n",
    "avec la commande `LinearConstraint(Phi, a, b)`. A vous de compléter la commande suivante, avec les objets définis à la question **1)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrainte = LinearConstraint(# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** On peut maintenant obtenir la solution de notre problème, en faisant appel à `scipy.optimize.minimize` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sol = minimize(                          # minimiser\n",
    "                    fun = objectif,           # la fonction 'objectif'\n",
    "                    constraints = contrainte, # sous la contrainte 'contrainte'\n",
    "                    x0 = np.random.randn(3),  # en partant d'un point initial\n",
    "                    ).x                       # et donne-moi le vecteur solution\n",
    "w_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprendre le code de la partie **I.1**, pour afficher les nuages de points ainsi que la droite définie par le paramètre $w_{sol} \\in \\mathbb{R}^3$ que vous venez d'obtenir. Êtes-vous satisfait$\\cdot$e de la solution obtenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# II. Classifier des images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Obtention des données : Importer le jeu de données MNIST\n",
    "\n",
    "On va résoudre un problème similaire, sauf que cette fois les données ne sont plus des points $x_i \\in \\mathbb{R}^2$ rouges ou bleus, mais des images $x_i \\in \\mathbb{R}^{64}$ de chiffres écrits à la main.\n",
    "On ne parlera donc plus de \"droite séparatrice\" mais d'\"hyperplan séparateur\".\n",
    "\n",
    "| Un nouveau jeu de données | \n",
    "| ----------- |\n",
    "| ![](images/mnist.jpg) |\n",
    "| Chaque image de $8\\times 8$ pixels est représentée par un point $x_i \\in \\mathbb{R}^{64}$ |\n",
    "\n",
    "Pour ce TP on va se contenter de séparer des images de 0 et de 1. Leurs étiquettes $y_i$ prendront respectivement les valeurs $-1$ et $+1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "plt.rcParams['image.cmap'] = 'gray_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits() # Importe un jeu de données à classer (10 classes)\n",
    "digits.data = digits.data*1/np.max(digits.data) # normalise : coeffs dans [0,1]\n",
    "classes = [0, 1] # Définit les 2 classes de nombres avec lesquelles on va travailler\n",
    "idx_classes = np.logical_or(digits.target == classes[0], digits.target == classes[1]) # Localise les deux classes ..\n",
    "digits.data = digits.data[idx_classes] # .. les extrait ..\n",
    "digits.target = digits.target[idx_classes] # .. et jette le reste\n",
    "digits.target = np.where(digits.target==classes[0],-1, 1) # Transforme les étiquettes de classes en {-1,+1}\n",
    "\n",
    "X, X_test, Y, Y_test = sklearn.model_selection.train_test_split( # On coupe le jeu de données en deux\n",
    "                        digits.data, digits.target, train_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche les 25 premières images de X\n",
    "fig, axs = plt.subplots(5, 5, figsize=(3, 3))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        k = i*5 + j\n",
    "        _ = axs[i,j].imshow(X[k].reshape(8,8))\n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** On dispose de données `X`, qui contient des images de 0 et 1 écrits à la main. Plus précisément, pour tout `k`, `X[k]` représente une telle image 2D (8x8 pixels) qui a été aplatie en un vecteur 1D.\n",
    "\n",
    "Utiliser `.shape` pour déterminer le nombre d'images que contient `X`.\n",
    "\n",
    "Prendre une image au hasard, et l'afficher avec la fonction `plt.imshow()`. Afin de l'afficher, vous aurez besoin de temporairement remettre cette image sous forme 2D avec la méthode `.reshape(nb_lignes, nb_colonnes)` . Est-ce l'image d'un 0 ou d'un 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** On dispose d'étiquettes `Y` qui encodent la nature des images contenues dans `X`. Plus précisément, `Y[k]` contient `-1` si `X[k]` est l'image d'un 0, ou `+1` si `X[k]` est l'image d'un 1.\n",
    "\n",
    "Reconsidérer l'image affichée à la question précédente, et vérifier que son étiquette correspond à ce que vous avez observé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Phase d'entraînement : Trouver un classifieur avec la méthode SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0)** Définir `m` le nombre de données dans notre jeu de données ; et `d` la taille de chacune de ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = \n",
    "d = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Nous allons trouver un classifieur linéaire avec la méthode SVM, comme au I.\n",
    "\n",
    "Pour ce faire, définir deux vecteurs `e`$= (1, \\dots , 1)^\\top$, `infini`$=(+\\infty, \\dots, +\\infty)^\\top$ de $\\mathbb{R}^m$, ainsi que la matrice $\\Phi \\in \\mathcal{M}_{m,d+1}(\\mathbb{R})$ définie en Section I.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Définir une fonction `objectif` qui prend en entrée un vecteur $w=(a,b) \\in \\mathbb{R}^{d} \\times \\mathbb{R}$ et qui renvoie $\\Vert a \\Vert^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectif(w):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Définir `C`, qui représente la contrainte linéaire $e \\preceq \\Phi w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Utiliser la fonction `minimize` de `scipy.optimize` pour trouver `w`, le minimiseur de `objectif` sous la contrainte `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** Ecrire une fonction `classifieur`, qui:\n",
    "- prend en entrée une image aplatie `x`$\\in \\mathbb{R}^d$\n",
    "- prend en entrée un vecteur de paramètres  `w`$=(a,b)\\in \\mathbb{R}^d \\times \\mathbb{R}$\n",
    "- retourne +1 si $\\langle a, x \\rangle + b \\geq 0$, -1 sinon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifieur(x, w):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Prendre une image au hasard dans `X`, et comparer sa vraie étiquette avec la prédiction faite par notre nouveau classifieur. Notre prédiction est elle bonne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase de test : connaitre la vraie performance de notre classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vient de voir que notre classifieur marche bien lorsque on l'applique aux images contenues dans `X`. Or ceci n'est pas très surprenant : le classifieur a été construit à partir de `w_MNIST`, la solution d'un problème d'optimisation dépendant des données contenues dans `X,Y`. Pour vraiment déterminer si notre modèle a **appris** quelque chose, il faut le tester sur des données qu'il n'a encore **jamais vues**.\n",
    "\n",
    "On va donc maintenant utiliser les données de test `X_test` et `Y_test` que l'on a pas encore utilisées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Calculer le pourcentage de bonnes réponses données par notre `classifieur`. Autrement dit, vous allez parcourir l'ensemble des données de test, et calculer le pourcentage du nombre de fois que le classifieur donne la bonne prédiction, en la comparant à la vraie étiquette contenue dans `Y_test`. Que pensez-vous du nombre obtenu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Si le taux de bonne réponse n'est pas de 100%, essayez de trouver dans le jeu de données quelles sont les images sur lesquelles le classifieur se trompe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Si vous retournez au début de la section **II.**, vous pouvez voir qu'au moment de l'importation nous avons demandé à travailler avec les `classes` 0 et 1.\n",
    "\n",
    "Remplacez ces chiffres par deux autres chiffres de votre choix, et relancez votre code afin de déterminer le taux de bonne réponse du classifieur. Essayez de prendre des chiffres difficiles à classer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Pour aller plus loin : Classification multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "On a vu comment classer des données à deux étiquettes. Mais en pratique il y a souvent un plus grand nombre de classes : par exemple MNIST peut contenir jusqu'à 10 classes: les chiffres de 0 à 9! Un tel classifieur vous est par exemple mis à disposition sur [ce site](https://mco-mnist-draw-rwpxka3zaa-ue.a.run.app/), qui vous permet de dessiner en ligne un chiffre et vous fournira en temps réel une estimation de la probabilité d'appartenance à une classe de chiffre.\n",
    "\n",
    "Je vous propose dans cette section de construire un tel classifieur. Notre stratégie va consister à réunir plusieurs classifieurs à deux classes (que vous avez appris à construire dans la section précédente) pour construire un classifieur à 10 classes. \n",
    "Plus précisément notre stratégie sera:\n",
    "\n",
    "- Couper le jeu de données en deux classes : les 0 et le reste (1, ..., 9). On produit alors un classifieur qui sera capable de savoir si une image est un zéro, ou non.\n",
    "- On refait la même chose en isolant cette fois 1 versus le reste (0,2, ..., 9). Et ainsi de suite. Ce qui nous donnera 10 classifieurs, chacun répondant à la question \"est-ce que cette image est un 0? un 1? etc.\n",
    "- Etant donné une nouvelle image, on la passe dans les 10 classifieurs, et en fonction des 10 prédictions on prend une décision.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manipulation des données et construction du premier classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "from scipy.optimize import LinearConstraint, minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "plt.rcParams['xtick.labelbottom'] = True\n",
    "plt.rcParams['ytick.labelleft'] = True\n",
    "plt.rcParams['image.cmap'] = 'gray_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits() # Importe un jeu de données à classer (10 classes)\n",
    "digits.data = digits.data*1/np.max(digits.data) # normalise : coeffs dans [0,1]\n",
    "X, X_test, Y, Y_test = sklearn.model_selection.train_test_split( # On coupe le jeu de données en deux\n",
    "                        digits.data, digits.target, train_size=0.25, shuffle=True)\n",
    "etiquettes_original = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Déterminer `m` le nombre de données contenues dans `X`, et `d` la dimension de ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = \n",
    "d = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Vérifier que `Y` contient bien des étiquettes allant de 0 à 9. Dans le but de classer les 0 vs. le reste, créez un nouveau vecteur d'étiquettes `Y_temp` qui vaut +1 pour les 0, et -1 pour le reste. Faites bien attention à ne pas modifier le `Y` original!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** En vous inspirant de ce que vous avez fait à la section précédente:\n",
    "\n",
    "- définissez le problème de SVM associé au problème de classifier 0 vs. le reste\n",
    "- résolvez-le, afin d'obtenir `w` un vecteur de paramètres définissant un hyperplan qui sépare les 0 des autres chiffres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectif(w):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Prenez quelques examples dans `X` et vérifiez que `w` définit un hyperplan qui sépare bien les 0 du reste (cf. section précédente). Rappelez-vous que les vraies étiquettes sont contenues dans `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction d'un classifieur général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Ici vous devrez reproduire ce que vous venez de faire :\n",
    "\n",
    "- Pour chaque chiffre 'cible' entre 0 et 9:\n",
    "  - Définir un vecteur d'étiquettes décrivant un problème de classification 'cible' vs. le reste\n",
    "  - Définir le problème SVM associé et le résoudre, ce qui vous donnera un vecteurs de paramètres w\n",
    "  - Ranger chacun de ces vecteurs $w \\in \\mathbb{R}^{d+1}$ comme ligne d'une matrice $W \\in \\mathcal{M}_{10, d+1}(\\mathbb{R})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Vérifiez que la matrice `W` obtenue donne de bons résultats. Pour cela, pous pourrez prendre une image quelconque, et la passer dans le classifieur binaire pour chaque ligne de `W` : il devrait renvoyer +1 seulement pour le bon indice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Définir une fonction `classifieur_general` qui:\n",
    "\n",
    "- prend en entrée une donnée $x$ et une matrice de paramètres  $W \\in \\mathcal{M}_{10, d+1}(\\mathbb{R})$\n",
    "- pour chaque chiffre `i` entre 0 et 9, utilise le classifieur binaire induit par `W[i,:]` pour déterminer si $x$ est égal à `i` ou non\n",
    "- renvoie `i` si $x$ est égal à `i`\n",
    "- pensez à faire en sorte que la fonction renvoie toujours quelque chose\n",
    "\n",
    "Vous testerez sur un exemple que cela marche bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifieur_general(x, W):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Appliquez votre `classifieur_general` aux données de test, comme au **II.3.1)**. Quel est le pourcentage de bonnes réponses de votre classifieur? Que pensez-vous du résultat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construction d'un classifieur général amélioré\n",
    "\n",
    "Le classifieur précédent souffre de quelques défauts.\n",
    "Par exemple, si une image est classée comme appartenant à deux classes, comment choisir laquelle des deux est la meilleure?\n",
    "Et si une image apparait classée comme appartenant à aucune classe, comment choisir quelle classe est la moins mauvaise?\n",
    "On donc besoin que les classifieurs binaires renvoient un peu plus que une réponse $\\pm 1$ pour chaque chiffre : il nous faut également un indice de confiance, une *probabilité* que la réponse $\\pm 1$ soit correcte.\n",
    "\n",
    "Par exemple, imaginons que le classifieur biniaire \"0 vs. le reste\" nous dise que $\\mathbb{P}(x=0) = 0.4$. Dans ce cas on pourrait conclure que $x$ n'est pas un 0, puisque $\\mathbb{P}(x\\neq 0) = 0.6$. Mais si tous les autres classifieurs binaires nous disent également que $\\mathbb{P}(x=k) = 0.01$, alors on pourrait se dire que 0 est la moins mauvaise réponse.\n",
    "\n",
    "Pour notre problème, cette probabilité va être liée à la distance entre la donnée $x$ et l'hyperplan séparateur $H_w$ : plus la donnée est proche de l'hyperplan, et moins on aura confiance en la prédiction, donc plus basse sera la probabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Définir une fonction `classifieur_general2` qui reprend le principe de `classifieur_general`. Cette fois-ci, l'indice `i` renvoyé sera selui qui maximise la probabilité $p_i$, où $p \\in \\mathbb{R}^{10}$ est défini par :\n",
    "\n",
    "$$ p_i := \\frac{e^{v_i}}{\\sum\\limits_{j=1}^p e^{v_j}},\n",
    "\\quad\n",
    "v_i := \\langle a^i, x \\rangle + b^i\n",
    "$$\n",
    "avec `W[i,:]`$=(a^i, b^i)$. \n",
    "Vous testerez sur un exemple que votre fonction marche bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Estimer la performance de votre nouveau classifieur sur les données de test. Comparer avec le classifieur précédent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour aller plus loin:** \n",
    "\n",
    "- Rien ne vous empêche de dessiner vos propres chiffres, et de tester si votre classifieur le reconnait.. Pour cela il vous suffit de produire une image carrée, de la redimensionner en image 8x8, puis de l'importer dans python.\n",
    "- Il existe d'autres façons de faire de la classification multiclasse. C'est simplement et rapidement expliqué dans [ce cours en ligne](https://openclassrooms.com/fr/courses/4444646-entrainez-un-modele-predictif-lineaire/4507846-classifiez-vos-donnees-en-plus-de-deux-classes) de Chloé-Agathe Azencott."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Ressources utilisées pour charger et utiliser MNIST:\n",
    "\n",
    "- https://dmkothari.github.io/Machine-Learning-Projects/SVM_with_MNIST.html\n",
    "- https://towardsdatascience.com/support-vector-machine-mnist-digit-classification-with-python-including-my-hand-written-digits-83d6eca7004a\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
